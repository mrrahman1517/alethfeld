% Alethfeld Generated Proof
% Graph: graph-18d478-4253a9 v21
% Theorem: PAC-Bayes Generalization Bound
% Generated: 2026-01-05

\documentclass[11pt,a4paper]{article}

\usepackage{amsmath,amssymb,amsthm}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{hyperref}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

% Lamport-style proof steps
\newcounter{proofstep}
\newcounter{proofsubstep}[proofstep]

\newenvironment{proofsteps}{%
  \setcounter{proofstep}{0}%
  \begin{list}{}{%
    \setlength{\leftmargin}{2em}%
    \setlength{\itemsep}{0.5em}%
  }%
}{\end{list}}

\newcommand{\step}[1]{%
  \stepcounter{proofstep}%
  \item[\textbf{$\langle 1 \rangle$\arabic{proofstep}.}] #1%
}

\newcommand{\substep}[1]{%
  \stepcounter{proofsubstep}%
  \item[\textbf{$\langle 2 \rangle$\arabic{proofsubstep}.}] #1%
}

% Justification margin note
\newcommand{\by}[1]{\hfill\textcolor{gray}{\small[#1]}}

% Status markers
\newcommand{\verified}{\textcolor{green!60!black}{\checkmark~VERIFIED}}

% Math operators
\DeclareMathOperator{\KL}{KL}
\DeclareMathOperator{\evidence}{evidence}

\title{PAC-Bayes Generalization Bound:\\A Complete Self-Contained Proof}
\author{Generated by Alethfeld v5.1}
\date{January 5, 2026}

\begin{document}

\maketitle

\begin{abstract}
We derive a PAC-Bayes bound on the expected generalization error for hypotheses 
that achieve zero training error. The bound relates the expected population loss 
to the KL divergence between posterior and prior. This proof is \textbf{fully 
self-contained}: we prove the core Gibbs variational inequality from the 
non-negativity of KL divergence.
\end{abstract}

\section{Setup}

\begin{definition}[Weight Space and Distributions]
Let $\mathcal{W}$ be the weight space of a learning algorithm. We define:
\begin{itemize}
    \item $P$: prior distribution on $\mathcal{W}$
    \item $\mathcal{S} = \{w \in \mathcal{W} : \text{train\_error}(w) = 0\}$: the set of solutions
    \item $Q$: posterior distribution, defined as $P$ restricted to $\mathcal{S}$
    \item $h(w)$: generalization error (population loss) of weight vector $w$
    \item $\beta = m - 1$ where $m$ is the number of training points
\end{itemize}
\end{definition}

\section{Core Lemma: Gibbs Variational Inequality}

\begin{lemma}[Change of Measure / Donsker-Varadhan]
\label{lem:gibbs}
Let $P$ and $Q$ be distributions on the same space with $Q \ll P$. Let $h(w)$ be 
any measurable function such that $\mathbb{E}_{w \sim P}[e^{\beta h(w)}] < \infty$ 
for some $\beta > 0$. Then:
\[
\mathbb{E}_{w \sim Q}[h(w)] \leq \frac{\KL(Q \| P) + \ln \mathbb{E}_{w \sim P}[e^{\beta h(w)}]}{\beta}
\]
\end{lemma}

\begin{proof}
We prove this from the non-negativity of KL divergence.

\begin{list}{}{\setlength{\leftmargin}{2em}\setlength{\itemsep}{0.5em}}

\item[\textbf{$\langle 2 \rangle$1.}] Define the \textbf{tilted distribution}:
\[
\tilde{P}(w) = \frac{e^{\beta h(w)} P(w)}{Z}, \quad \text{where } Z = \mathbb{E}_{w \sim P}[e^{\beta h(w)}]
\]
\by{definition}

\item[\textbf{$\langle 2 \rangle$2.}] $\tilde{P}$ is a valid probability distribution since:
\[
\int \tilde{P}(w)\, dw = \frac{1}{Z} \int e^{\beta h(w)} P(w)\, dw = \frac{Z}{Z} = 1
\]
\by{algebraic; from $\langle 2 \rangle$1}

\item[\textbf{$\langle 2 \rangle$3.}] By non-negativity of KL divergence:
\[
\KL(Q \| \tilde{P}) \geq 0
\]
\by{known result; from $\langle 2 \rangle$2}

\item[\textbf{$\langle 2 \rangle$4.}] Expanding the KL divergence:
\[
\KL(Q \| \tilde{P}) = \mathbb{E}_{w \sim Q}\left[\ln \frac{Q(w)}{\tilde{P}(w)}\right] 
= \mathbb{E}_{w \sim Q}\left[\ln Q(w) - \ln \tilde{P}(w)\right]
\]
\by{definition; from $\langle 2 \rangle$3}

\item[\textbf{$\langle 2 \rangle$5.}] From the definition of $\tilde{P}$:
\[
\ln \tilde{P}(w) = \beta h(w) + \ln P(w) - \ln Z
\]
\by{substitution; from $\langle 2 \rangle$1, $\langle 2 \rangle$4}

\item[\textbf{$\langle 2 \rangle$6.}] Therefore:
\begin{align*}
\KL(Q \| \tilde{P}) &= \mathbb{E}_{w \sim Q}\left[\ln Q(w) - \beta h(w) - \ln P(w) + \ln Z\right] \\
&= \KL(Q \| P) - \beta \mathbb{E}_{w \sim Q}[h(w)] + \ln Z
\end{align*}
\by{algebraic; from $\langle 2 \rangle$4, $\langle 2 \rangle$5}

\item[\textbf{$\langle 2 \rangle$7.}] From $\KL(Q \| \tilde{P}) \geq 0$:
\[
\KL(Q \| P) - \beta \mathbb{E}_{w \sim Q}[h(w)] + \ln Z \geq 0
\]
\by{modus ponens; from $\langle 2 \rangle$3, $\langle 2 \rangle$6}

\item[\textbf{$\langle 2 \rangle$8.}] Rearranging:
\[
\beta \mathbb{E}_{w \sim Q}[h(w)] \leq \KL(Q \| P) + \ln Z
\]
Dividing by $\beta > 0$:
\[
\mathbb{E}_{w \sim Q}[h(w)] \leq \frac{\KL(Q \| P) + \ln \mathbb{E}_{w \sim P}[e^{\beta h(w)}]}{\beta} \qquad \qed
\]
\by{algebraic; from $\langle 2 \rangle$7}

\end{list}
\end{proof}

\section{Main Result}

\begin{theorem}[PAC-Bayes Bound for Zero Training Error]
\label{thm:pac-bayes}
Under the setup above:
\[
\mathbb{E}_{w \sim Q}[h(w)] \leq \frac{\KL(Q \| P) + \ln \mathbb{E}_{w \sim P}[e^{\beta h(w)}]}{\beta}
\]
where for posterior $Q$ restricted to solutions:
\[
\KL(Q \| P) = \ln \frac{1}{\evidence}
\]
\end{theorem}

\begin{proof}
We apply Lemma~\ref{lem:gibbs} with specific choices for the posterior.

\begin{proofsteps}

\step{\textbf{Assumption.} Let $P$ be a prior distribution on the weight space $\mathcal{W}$.}
\by{assumption}
\label{step:1}

\step{\textbf{Assumption.} Let $Q$ be the posterior distribution, defined as $P$ restricted 
to the set of solutions $\mathcal{S} = \{w : \text{train\_error}(w) = 0\}$.}
\by{assumption}
\label{step:2}

\step{\textbf{Assumption.} Let $h(w)$ denote the generalization error (population loss) 
of weight vector $w$.}
\by{assumption}
\label{step:3}

\step{\textbf{Assumption.} Let $\beta = m - 1$ where $m$ is the number of training points.}
\by{assumption}
\label{step:4}

\step{\textbf{Gibbs Variational Inequality} (Lemma~\ref{lem:gibbs}): For any prior $P$, posterior $Q$, 
and $\beta > 0$:
\[
\mathbb{E}_{w \sim Q}[h(w)] \leq \frac{\KL(Q \| P) + \ln \mathbb{E}_{w \sim P}[e^{\beta h(w)}]}{\beta}
\]
}
\by{\verified; proved above}
\label{step:5}

\step{Since $Q$ is $P$ restricted to solutions $\mathcal{S}$, we have:
\[
Q(w) = \frac{P(w)}{P(\mathcal{S})} \quad \text{for } w \in \mathcal{S}, \qquad Q(w) = 0 \text{ otherwise}
\]
}
\by{definition; from \ref{step:1}, \ref{step:2}}
\label{step:6}

\step{Computing the KL divergence:
\begin{align*}
\KL(Q \| P) &= \mathbb{E}_{w \sim Q}\left[\ln \frac{Q(w)}{P(w)}\right] \\
&= \mathbb{E}_{w \sim Q}\left[\ln \frac{P(w)/P(\mathcal{S})}{P(w)}\right] \\
&= \mathbb{E}_{w \sim Q}\left[\ln \frac{1}{P(\mathcal{S})}\right] \\
&= \ln \frac{1}{P(\mathcal{S})}
\end{align*}
}
\by{algebraic; from \ref{step:6}}
\label{step:7}

\step{$P(\mathcal{S})$ is the prior probability of all solutions, i.e., the \textbf{evidence}. 
Thus:
\[
\KL(Q \| P) = \ln \frac{1}{\evidence}
\]
}
\by{definition; from \ref{step:7}}
\label{step:8}

\step{For $w \in \text{supp}(Q) = \mathcal{S}$, the training error is 0, so $h(w)$ measures 
the population (generalization) error directly.}
\by{modus ponens; from \ref{step:2}, \ref{step:3}}
\label{step:9}

\step{Applying Lemma~\ref{lem:gibbs} with our setup:
\[
\mathbb{E}_{w \sim Q}[h(w)] \leq \frac{\ln(1/\evidence) + \ln \mathbb{E}_{w \sim P}[e^{(m-1) h(w)}]}{m-1}
\]
}
\by{substitution; from \ref{step:4}, \ref{step:5}, \ref{step:8}}
\label{step:10}

\step{\textbf{QED.} The expected generalization error over solutions is bounded by the sum 
of log-inverse-evidence and the log-moment-generating-function of the loss under the prior, 
normalized by $\beta = m-1$.}
\by{from \ref{step:9}, \ref{step:10}}

\end{proofsteps}
\end{proof}

\section{Interpretation}

The bound decomposes into two terms:

\begin{enumerate}
    \item \textbf{Complexity term}: $\ln(1/\evidence) = -\ln P(\mathcal{S})$
    
    This measures how ``surprising'' it is that a random weight vector from the prior 
    achieves zero training error. Smaller solution sets (more complex hypotheses) 
    yield larger complexity penalties.
    
    \item \textbf{Prior MGF term}: $\ln \mathbb{E}_{w \sim P}[e^{\beta h(w)}]$
    
    This is the log-moment-generating-function of the generalization error under the prior. 
    It captures how well the prior concentrates on low-error hypotheses.
\end{enumerate}

\section{Proof Metadata}

\begin{tabular}{ll}
\textbf{Graph ID:} & \texttt{graph-18d478-4253a9} \\
\textbf{Version:} & 21 \\
\textbf{Proof Mode:} & strict-mathematics \\
\textbf{Total Steps:} & 19 \\
\textbf{Assumptions:} & 5 \\
\textbf{Claims:} & 13 \\
\textbf{Verified:} & 1 (Gibbs lemma) \\
\textbf{Taint Status:} & \textbf{Clean} (fully self-contained) \\
\end{tabular}

\section{References}

\begin{itemize}
    \item Donsker, M. D. \& Varadhan, S. R. S. (1975). Asymptotic evaluation of certain 
          Markov process expectations for large time. \textit{Comm. Pure Appl. Math.}
    \item McAllester, D. A. (1999). PAC-Bayesian model averaging. 
          \textit{Proceedings of the 12th Annual Conference on Computational Learning Theory}.
    \item Catoni, O. (2007). \textit{PAC-Bayesian Supervised Classification}. 
          Institute of Mathematical Statistics Lecture Notes.
\end{itemize}

\vspace{1em}

\noindent\textit{Generated by Alethfeld Proof Orchestrator v5.1}

\end{document}
