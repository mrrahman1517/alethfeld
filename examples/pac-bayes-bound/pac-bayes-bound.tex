% Alethfeld Generated Proof
% Graph: graph-18d478-4253a9 v12
% Theorem: PAC-Bayes Generalization Bound
% Generated: 2026-01-05

\documentclass[11pt,a4paper]{article}

\usepackage{amsmath,amssymb,amsthm}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{hyperref}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

% Lamport-style proof steps
\newcounter{proofstep}
\newcounter{proofsubstep}[proofstep]

\newenvironment{proofsteps}{%
  \setcounter{proofstep}{0}%
  \begin{list}{}{%
    \setlength{\leftmargin}{2em}%
    \setlength{\itemsep}{0.5em}%
  }%
}{\end{list}}

\newcommand{\step}[1]{%
  \stepcounter{proofstep}%
  \item[\textbf{$\langle 1 \rangle$\arabic{proofstep}.}] #1%
}

% Justification margin note
\newcommand{\by}[1]{\hfill\textcolor{gray}{\small[#1]}}

% Status markers
\newcommand{\admitted}{\textcolor{orange}{$\star$~ADMITTED}}

% Math operators
\DeclareMathOperator{\KL}{KL}
\DeclareMathOperator{\evidence}{evidence}

\title{PAC-Bayes Generalization Bound:\\A Derivation Sketch}
\author{Generated by Alethfeld v5.1}
\date{January 5, 2026}

\begin{document}

\maketitle

\begin{abstract}
We derive a PAC-Bayes bound on the expected generalization error for hypotheses 
that achieve zero training error. The bound relates the expected population loss 
to the KL divergence between posterior and prior, which equals the negative log 
of the Bayesian evidence.
\end{abstract}

\section{Setup}

\begin{definition}[Weight Space and Distributions]
Let $\mathcal{W}$ be the weight space of a learning algorithm. We define:
\begin{itemize}
    \item $P$: prior distribution on $\mathcal{W}$
    \item $\mathcal{S} = \{w \in \mathcal{W} : \text{train\_error}(w) = 0\}$: the set of solutions
    \item $Q$: posterior distribution, defined as $P$ restricted to $\mathcal{S}$
    \item $h(w)$: generalization error (population loss) of weight vector $w$
    \item $\beta = m - 1$ where $m$ is the number of training points
\end{itemize}
\end{definition}

\section{Main Result}

\begin{theorem}[PAC-Bayes Bound for Zero Training Error]
\label{thm:pac-bayes}
Under the setup above:
\[
\mathbb{E}_{w \sim Q}[h(w)] \leq \frac{\KL(Q \| P) + \ln \mathbb{E}_{w \sim P}[e^{\beta h(w)}]}{\beta}
\]
\end{theorem}

\begin{proof}
We derive this bound by applying the general PAC-Bayes theorem with specific choices 
for the prior and posterior.

\begin{proofsteps}

\step{\textbf{Assumption.} Let $P$ be a prior distribution on the weight space $\mathcal{W}$.}
\by{assumption}
\label{step:1}

\step{\textbf{Assumption.} Let $Q$ be the posterior distribution, defined as $P$ restricted 
to the set of solutions $\mathcal{S} = \{w : \text{train\_error}(w) = 0\}$.}
\by{assumption}
\label{step:2}

\step{\textbf{Assumption.} Let $h(w)$ denote the generalization error (population loss) 
of weight vector $w$.}
\by{assumption}
\label{step:3}

\step{\textbf{Assumption.} Let $\beta = m - 1$ where $m$ is the number of training points.}
\by{assumption}
\label{step:4}

\step{\textbf{PAC-Bayes Theorem} (McAllester, 1999): For any prior $P$, posterior $Q$, 
and $\beta > 0$:
\[
\mathbb{E}_{w \sim Q}[h(w)] \leq \frac{\KL(Q \| P) + \ln \mathbb{E}_{w \sim P}[e^{\beta h(w)}]}{\beta}
\]
}
\by{\admitted}
\label{step:5}

This is a standard result in PAC-Bayes theory. See McAllester (1999) or 
Catoni (2007) for proofs.

\step{Since $Q$ is $P$ restricted to solutions $\mathcal{S}$, we have:
\[
Q(w) = \frac{P(w)}{P(\mathcal{S})} \quad \text{for } w \in \mathcal{S}, \qquad Q(w) = 0 \text{ otherwise}
\]
}
\by{definition; from \ref{step:1}, \ref{step:2}}
\label{step:6}

\step{Computing the KL divergence:
\begin{align*}
\KL(Q \| P) &= \mathbb{E}_{w \sim Q}\left[\ln \frac{Q(w)}{P(w)}\right] \\
&= \mathbb{E}_{w \sim Q}\left[\ln \frac{P(w)/P(\mathcal{S})}{P(w)}\right] \\
&= \mathbb{E}_{w \sim Q}\left[\ln \frac{1}{P(\mathcal{S})}\right] \\
&= \ln \frac{1}{P(\mathcal{S})}
\end{align*}
}
\by{algebraic; from \ref{step:6}}
\label{step:7}

\step{$P(\mathcal{S})$ is the prior probability of all solutions, i.e., the \textbf{evidence}. 
Thus:
\[
\KL(Q \| P) = \ln \frac{1}{\evidence}
\]
}
\by{definition; from \ref{step:7}}
\label{step:8}

\step{For $w \in \text{supp}(Q) = \mathcal{S}$, the training error is 0, so $h(w)$ measures 
the population (generalization) error directly.}
\by{modus ponens; from \ref{step:2}, \ref{step:3}}
\label{step:9}

\step{Applying the PAC-Bayes theorem with our setup:
\[
\mathbb{E}_{w \sim Q}[h(w)] \leq \frac{\ln(1/\evidence) + \ln \mathbb{E}_{w \sim P}[e^{(m-1) h(w)}]}{m-1}
\]
}
\by{substitution; from \ref{step:4}, \ref{step:5}, \ref{step:8}}
\label{step:10}

\step{\textbf{QED.} The expected generalization error over solutions is bounded by the sum 
of log-inverse-evidence and the log-moment-generating-function of the loss under the prior, 
normalized by $\beta = m-1$.}
\by{from \ref{step:9}, \ref{step:10}}

\end{proofsteps}
\end{proof}

\section{Interpretation}

The bound decomposes into two terms:

\begin{enumerate}
    \item \textbf{Complexity term}: $\ln(1/\evidence) = -\ln P(\mathcal{S})$
    
    This measures how ``surprising'' it is that a random weight vector from the prior 
    achieves zero training error. Smaller solution sets (more complex hypotheses) 
    yield larger complexity penalties.
    
    \item \textbf{Prior MGF term}: $\ln \mathbb{E}_{w \sim P}[e^{\beta h(w)}]$
    
    This is the log-moment-generating-function of the generalization error under the prior. 
    It captures how well the prior concentrates on low-error hypotheses.
\end{enumerate}

\section{Proof Metadata}

\begin{tabular}{ll}
\textbf{Graph ID:} & \texttt{graph-18d478-4253a9} \\
\textbf{Version:} & 12 \\
\textbf{Proof Mode:} & strict-mathematics \\
\textbf{Total Steps:} & 11 \\
\textbf{Assumptions:} & 4 \\
\textbf{Claims:} & 6 \\
\textbf{Admitted Steps:} & 1 (PAC-Bayes Theorem) \\
\textbf{Taint Status:} & Tainted (depends on admitted step) \\
\end{tabular}

\section{References}

\begin{itemize}
    \item McAllester, D. A. (1999). PAC-Bayesian model averaging. 
          \textit{Proceedings of the 12th Annual Conference on Computational Learning Theory}.
    \item Catoni, O. (2007). \textit{PAC-Bayesian Supervised Classification}. 
          Institute of Mathematical Statistics Lecture Notes.
\end{itemize}

\vspace{1em}

\noindent\textit{Generated by Alethfeld Proof Orchestrator v5.1}

\end{document}

